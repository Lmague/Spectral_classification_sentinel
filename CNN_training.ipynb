{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj8klGbEW4lW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Reproductibilité\n",
        "tf.keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    SAVE_PATH = '/content/drive/MyDrive/ModelCNN_WEBIA.keras'\n",
        "except ImportError:\n",
        "    SAVE_PATH = 'ModelCNN_WEBIA.keras'\n",
        "\n",
        "print(\"Model will be saved to:\", SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "XTMhE7VdW890"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = tfds.builder(\"eurosat\")"
      ],
      "metadata": {
        "id": "KAQCglztW_hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants configuration\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_SIZE = 64\n",
        "\n",
        "print(\"Loading EuroSAT dataset...\")\n",
        "\n",
        "# 1. Dataset Loading (RGB version)\n",
        "ds, info = tfds.load(\"eurosat/rgb\", split=\"train\", with_info=True, as_supervised=True)\n",
        "NUM_CLASSES = info.features[\"label\"].num_classes\n",
        "\n",
        "# 2. Global Shuffle (Crucial before split)\n",
        "ds = ds.shuffle(10000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "# 3. Calculate Split Sizes (70/15/15)\n",
        "total_examples = info.splits[\"train\"].num_examples\n",
        "n_train = int(0.7 * total_examples)\n",
        "n_val = int(0.15 * total_examples)\n",
        "n_test = total_examples - n_train - n_val\n",
        "\n",
        "print(\"Total:\", total_examples, \"| Train:\", n_train, \"| Val:\", n_val, \"| Test:\", n_test)\n",
        "\n",
        "# 4. Split datasets\n",
        "train_raw = ds.take(n_train)\n",
        "rest = ds.skip(n_train)\n",
        "val_raw = rest.take(n_val)\n",
        "test_raw = rest.skip(n_val)\n",
        "\n",
        "# 5. Preprocess function\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    return image, label\n",
        "\n",
        "# 6. Build data pipelines\n",
        "train_ds = (train_raw\n",
        "            .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .cache()\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "val_ds = (val_raw\n",
        "          .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "          .batch(BATCH_SIZE)\n",
        "          .cache()\n",
        "          .prefetch(AUTOTUNE))\n",
        "\n",
        "test_ds = (test_raw\n",
        "           .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "           .batch(BATCH_SIZE)\n",
        "           .cache()\n",
        "           .prefetch(AUTOTUNE))\n",
        "\n",
        "# 7. Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "print(\"Data pipeline ready.\")"
      ],
      "metadata": {
        "id": "x17SKTueXC19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters, stride=1):\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(filters, (3, 3), strides=stride, padding=\"same\",\n",
        "                      use_bias=False, kernel_initializer='he_normal')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('gelu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters, (3, 3), padding=\"same\",\n",
        "                      use_bias=False, kernel_initializer='he_normal')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    if stride > 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = layers.Conv2D(filters, (1, 1), strides=stride, padding=\"same\",\n",
        "                                 use_bias=False, kernel_initializer='he_normal')(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('gelu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\n",
        "\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Conv2D(64, (3, 3), padding=\"same\", use_bias=False, kernel_initializer='he_normal')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('gelu')(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Small ResNet-like stack\n",
        "x = residual_block(x, 64, stride=1)\n",
        "x = residual_block(x, 64, stride=1)\n",
        "\n",
        "x = residual_block(x, 128, stride=2)\n",
        "x = residual_block(x, 128, stride=1)\n",
        "\n",
        "x = residual_block(x, 256, stride=2)\n",
        "x = residual_block(x, 256, stride=1)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = models.Model(inputs, outputs, name=\"ModelCNN_WEBIA\")\n",
        "\n",
        "my_callbacks = [\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j2lkig6QXGB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=150,\n",
        "    callbacks=my_callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "CrbsxdVdXJEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Créer une figure avec deux sous-graphiques côte à côte\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Graphe de la perte\n",
        "axes[0].plot(history.history['loss'], label='Train Loss')\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Graphe de l'accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[1].set_title('Accuracy')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "model.save(SAVE_PATH)\n",
        "print(\"Saved model to:\", SAVE_PATH)"
      ],
      "metadata": {
        "id": "4erdCKDSXLor"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}